{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import load\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from gensim.models import KeyedVectors\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import no_grad\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation as punct\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n",
    "from tqdm import tqdm\n",
    "from re import sub\n",
    "from numpy import array, zeros\n",
    "from random import choice, seed\n",
    "seed(1509)\n",
    "# from seaborn import heatmap\n",
    "\n",
    "# for W2V sim\n",
    "\n",
    "nlp = load(\"ru_core_news_lg\")\n",
    "W2V_model = KeyedVectors.load_word2vec_format('ruwikiruscorpora_upos_cbow_300_10_2021/model.bin', binary=True)\n",
    "\n",
    "# WordNet data proved to be unnecessary\n",
    "# for WordNet\n",
    "\n",
    "from ruwordnet import RuWordNet\n",
    "wn = RuWordNet()\n",
    "freq_lemmas = set()\n",
    "with open('ruscorpora_freqdict.csv', encoding='utf-8') as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        freq_lemmas.add(line.split(';')[1].strip('\"'))\n",
    "\n",
    "# for deixis\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "personal = ['он', 'она', 'оно', 'они']\n",
    "\n",
    "# for BERT sim\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")\n",
    "BERT_model = AutoModel.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sem_sim(para, same=True):\n",
    "    \n",
    "    # apply spacy to get PoS tags\n",
    "    \n",
    "    doc = nlp(para)\n",
    "    para_POSed = []\n",
    "    for sent in doc.sents:\n",
    "        s_POSed_words = []\n",
    "        for i in sent: # feature PoS are of questionable value, ADV too \n",
    "            if i.pos_ in ['ADJ', 'NOUN', 'PROPN', 'VERB'] and \\\n",
    "            i.text not in stopwords.words('russian')\\\n",
    "            and i.text != '-':\n",
    "                s_POSed_words.append(i.lemma_ + '_' + i.pos_)            \n",
    "        para_POSed.append(s_POSed_words)\n",
    "    \n",
    "    # compute vector similarity\n",
    "    \n",
    "    sims = []\n",
    "    mem = {}\n",
    "    for i in range(len(para_POSed)): # sents\n",
    "        sent_comp_to_others = []\n",
    "        \n",
    "        for j in range(len(para_POSed)):\n",
    "            if para_POSed[i] == [] or para_POSed[j] == []: # avoid empty sents generated previously\n",
    "                sent_comp_to_others.append(0)\n",
    "            else:              \n",
    "                interim_sims = set()\n",
    "                for word in para_POSed[j]:\n",
    "                    \n",
    "                    try:\n",
    "                        W2V_model[word]\n",
    "                    except KeyError: # ascribe random vector to OOV\n",
    "                        if mem.get(word) == None:\n",
    "                            mem[word] = choice(W2V_model.index_to_key)\n",
    "                            word = mem[word]\n",
    "                        else:\n",
    "                            word = mem[word]\n",
    "                    \n",
    "                    for slovo in para_POSed[i]:\n",
    "                        try:\n",
    "                            W2V_model[slovo]\n",
    "                        except KeyError:\n",
    "                            if mem.get(slovo) == None:\n",
    "                                mem[slovo] = choice(W2V_model.index_to_key)\n",
    "                                slovo = mem[slovo]\n",
    "                            else:\n",
    "                                slovo = mem[slovo]\n",
    "                                \n",
    "                        interim_sims.add(W2V_model.similarity(word, slovo))\n",
    "                if same == False and i in interim_sims:\n",
    "                    interim_sims.remove(1) # ignore same words\n",
    "                sent_comp_to_others.append(round(max(interim_sims), 2))\n",
    "        \n",
    "        sims.append(sent_comp_to_others)\n",
    "    \n",
    "    return array(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_related_words(two_sents_l, same=True):\n",
    "    \n",
    "    # lemmatize sent pair\n",
    "    \n",
    "    sent_pair = []\n",
    "    for i in range(2):\n",
    "        lemmas = []\n",
    "        for word in nlp(two_sents_l[i]):\n",
    "            if word.pos_ in ['ADJ', 'NOUN', 'PROPN', 'VERB'] \\\n",
    "            and word.lemma_ not in freq_lemmas:\n",
    "                lemmas.append(word.lemma_)\n",
    "        sent_pair.append(lemmas)\n",
    "    \n",
    "    # get related words from sent_pair[0] to check in sent_pair[1]\n",
    "    \n",
    "    target_words = set()\n",
    "    for lemma in sent_pair[0]:\n",
    "        try:\n",
    "            for i in wn[lemma]: # diff senses of lemma\n",
    "                for j in i.synset.hyponyms:\n",
    "                    target_words.add(j.title.lower())\n",
    "                for j in i.synset.hypernyms:\n",
    "                    target_words.add(j.title.lower())\n",
    "                for j in i.synset.domains:\n",
    "                    target_words.add(j.title.lower())\n",
    "                for j in i.synset.domain_items:\n",
    "                    target_words.add(j.title.lower())\n",
    "                for j in i.synset.holonyms:\n",
    "                    target_words.add(j.title.lower())\n",
    "                for j in i.synset.meronyms:\n",
    "                    target_words.add(j.title.lower())\n",
    "                for j in i.synset.classes:\n",
    "                    target_words.add(j.title.lower())\n",
    "                for j in i.synset.instances:\n",
    "                    target_words.add(j.title.lower())\n",
    "                for j in i.synset.premises:\n",
    "                    target_words.add(j.title.lower())\n",
    "                for j in i.synset.conclusions:\n",
    "                    target_words.add(j.title.lower())\n",
    "                for j in i.synset.causes:\n",
    "                    target_words.add(j.title.lower())\n",
    "                for j in i.synset.effects:\n",
    "                    target_words.add(j.title.lower())\n",
    "                for j in i.synset.pos_synonyms:\n",
    "                    target_words.add(j.title.lower())\n",
    "                for j in i.synset.antonyms:\n",
    "                    target_words.add(j.title.lower())\n",
    "                for j in i.synset.related:\n",
    "                    target_words.add(j.title.lower())\n",
    "                for j in i.derivations:\n",
    "                    target_words.add(j.name.lower())\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    for lemma in sent_pair[1]:\n",
    "        for target in target_words:\n",
    "            if lemma in target.split(' '):\n",
    "                if same == True:\n",
    "#                     print('lemma', [lemma], 'target', [target])\n",
    "                    return True\n",
    "                else:\n",
    "                    if lemma == target:\n",
    "                        pass\n",
    "                    else:\n",
    "#                         print('lemma', [lemma], 'target', [target])\n",
    "                        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_text_deixis(sent_pair, init_thr=6, core=True):\n",
    "    \n",
    "    for i in range(len(sent_pair)):\n",
    "        sent_pair[i] = [w.strip(punct + '«»') for w in sent_pair[i].split(' ')]\n",
    "    \n",
    "    contains_deixis = False\n",
    "    for i, word in enumerate(sent_pair[1]):\n",
    "        \n",
    "        if i == init_thr: # init_thr - how many tokens we check from the beginning of sentence\n",
    "            break\n",
    "            \n",
    "        word = morph.parse(word)\n",
    "\n",
    "        if word[0].normal_form in personal:\n",
    "            if word[0].normal_form == 'они':\n",
    "                \n",
    "                for slovo in sent_pair[0]:\n",
    "                    for razbor in morph.parse(slovo):\n",
    "                        if core == True:\n",
    "                            if razbor.tag.number == 'plur' \\\n",
    "                            and (razbor.tag.case == 'nomn' or razbor.tag.case == 'accs'):\n",
    "#                                 print('word', word, 'anaphor', slovo)\n",
    "                                contains_deixis = True\n",
    "                                return contains_deixis\n",
    "                        else:\n",
    "                            if razbor.tag.number == 'plur':\n",
    "#                                 print('word', word, 'anaphor', slovo)\n",
    "                                contains_deixis = True\n",
    "                                return contains_deixis\n",
    "            else: # он, она, оно\n",
    "                \n",
    "                for slovo in sent_pair[0]:\n",
    "                    for razbor in morph.parse(slovo):\n",
    "                        if core == True:\n",
    "                            if razbor.tag.gender == word[0].tag.gender \\\n",
    "                            and (razbor.tag.case == 'nomn' or razbor.tag.case == 'accs'):\n",
    "#                                 print('word', word, 'anaphor', slovo)\n",
    "                                contains_deixis = True\n",
    "                                return contains_deixis\n",
    "                        else:\n",
    "                            if razbor.tag.number == 'plur':\n",
    "#                                 print('word', word, 'anaphor', slovo)\n",
    "                                contains_deixis = True\n",
    "                                return contains_deixis\n",
    "    \n",
    "    return contains_deixis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sent, tokenizer, model):\n",
    "        for s in [sent]:\n",
    "            encoded_input = tokenizer(s, padding=True, truncation=True, max_length=64, return_tensors='pt')\n",
    "            with no_grad():\n",
    "                model_output = model(**encoded_input)\n",
    "        \n",
    "        return model_output[0][0][0].numpy()\n",
    "\n",
    "def bert_sent_sim(para_list, deep=1):\n",
    "    sims = []\n",
    "    for sent in para_list:\n",
    "        sims.append(encode(sent, tokenizer, BERT_model))\n",
    "    return cos_sim(array(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_para(para, deep=1, sim_thr=0.4, b_sim_thr=0.78, ratio_thr=0.6, deix=6, core=True, same=True, crit=[0,2,3]):\n",
    "    \n",
    "    '''\n",
    "    Parameters:\n",
    "    para (str): paragraph that we check for coherence\n",
    "    deep (int): how many sentences backwards semantic similarity is checked (crit. 0 and 3)\n",
    "    sim_thr (float): words with W2V cosine similarity above this threshold are said to establish coherence (crit. 0)\n",
    "    b_sim_thr (float): sentences with BERT CLS cosine similarity above this threshold are said to be coherent (crit. 3)\n",
    "    ratio_thr (float): ratio of coherent sentences in paragraph above this threshold deems paragraph coherent\n",
    "    deix (int): number of words from the beginning of sentence that are checked for deixis (crit. 2)\n",
    "    core (bool): check if candidate for antecedent if in Nom or Acc case (crit. 2)\n",
    "    same (bool): allow same words and similarity 1 to be present in similarity matrix (criteria 0 and 1)\n",
    "    crit (list): this list specifies which criteria will be applied\n",
    "    '''\n",
    "    \n",
    "    # initial preparation\n",
    "    \n",
    "    para_list = sent_tokenize(para)\n",
    "    sent_coh_index = zeros([len(para_list), 4], dtype = int)\n",
    "    \n",
    "    # Criterion 0: similarity associates\n",
    "    \n",
    "    if 0 in crit:\n",
    "        arr = get_sem_sim(para, same)\n",
    "    #     heatmap(data = arr, annot = True, cmap = 'Blues')\n",
    "        for i in range(1, len(para_list)):\n",
    "            for j in range(i - deep, i):\n",
    "                if not j == -1:\n",
    "                    if arr[i][j] >= sim_thr:\n",
    "                        sent_coh_index[i][0] = 1\n",
    "    \n",
    "    # Criterion 1: related words\n",
    "    \n",
    "    if 1 in crit:\n",
    "        for i in range(1, len(para_list)):\n",
    "            sent_coh_index[i][1] = contains_related_words([para_list[i - 1], para_list[i]], same)\n",
    "    \n",
    "    # Criteroin 2: basic anaphora\n",
    "    \n",
    "    if 2 in crit:\n",
    "        if deix > 0:\n",
    "            for i in range(1, len(para_list)):\n",
    "                sent_coh_index[i][2] = contains_text_deixis([para_list[i - 1], para_list[i]], deix, core)\n",
    "    \n",
    "    # Criterion 3: BERT sent simmilarity\n",
    "    \n",
    "    if 3 in crit:\n",
    "        B_arr = bert_sent_sim(para_list, deep)\n",
    "    #     heatmap(data = B_arr, annot = True, cmap = 'Greens')\n",
    "\n",
    "        for i in range(1, len(para_list)):\n",
    "            for j in range(i - deep, i):\n",
    "                if not j == -1:\n",
    "                    if B_arr[i][j] >= b_sim_thr:\n",
    "                        sent_coh_index[i][3] = 1\n",
    "    \n",
    "    # final return\n",
    "    \n",
    "    ct = 0\n",
    "    for row in sent_coh_index:\n",
    "        if 1 in row:\n",
    "            ct += 1 # note that [0] sent never supplies 1\n",
    "    \n",
    "    if ct / len(para_list) >= ratio_thr:\n",
    "        verdict = True\n",
    "    else:\n",
    "        verdict = False\n",
    "    \n",
    "    return verdict, sent_coh_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " array([[0, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [1, 0, 1, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 1]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para1 = 'Всегда почему-то казалось, что океанская дорога между Старым и Новым Светом очень оживлена, что то и дело навстречу попадаются веселые пароходы, с музыкой и флагами. На самом же деле океан – это штука величественная и пустынная, и пароходик, который штормовал в четырехстах милях от Европы, был единственным кораблем, который мы встретили за пять дней пути. «Нормандия» раскачивалась медленно и важно. Она шла, почти не уменьшив хода, уверенно расшвыривая высокие волны, которые лезли на нее со всех сторон, и только иногда отвешивала океану равномерные поклоны. Это не было борьбой мизерного создания человеческих рук с разбушевавшейся стихией. Это была схватка равного с равным.'\n",
    "process_para(para1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " array([[0, 0, 0, 0],\n",
       "        [1, 0, 0, 1],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 1],\n",
       "        [1, 0, 0, 1],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para2 = 'Все задрожало на корме, где мы помещались. Дрожали палубы, стены, иллюминаторы, шезлонги, стаканы над умывальником, сам умывальник. Вибрация парохода была столь сильной, что начали издавать звуки даже такие предметы, от которых никак этого нельзя было ожидать. Впервые в жизни мы слышали, как звучит полотенце, мыло, ковер на полу, бумага на столе, занавески, воротничок, брошенный на кровать. Звучало и гремело все, что находилось в каюте. Достаточно было пассажиру на секунду задуматься и ослабить мускулы лица, как у него начинали стучать зубы. Всю ночь казалось, что кто-то ломится в двери, стучит в окна, тяжко хохочет. Мы насчитали сотню различных звуков, которые издавала наша каюта.'\n",
    "process_para(para2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_text_metric(\n",
    "    filename, deep=1, sim_thr=0.4, b_sim_thr=0.78, ratio_thr=0.6, deix=6, core=True, same=True, para=4, crit=[0,2,3]):\n",
    "    \n",
    "    '''\n",
    "    Parameters:\n",
    "    filename (str): path to text file that we assess\n",
    "    para (int): we only consider paragraphs that contain more sentences than this threshold\n",
    "    \n",
    "    Other parameters are used to transmit its parameters to process_para\n",
    "    '''\n",
    "    \n",
    "    # preprocessing\n",
    "    \n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    text = sub('\\n{2,}', '\\n', text)\n",
    "    \n",
    "    text = text.split('\\n')\n",
    "    t_paras_as_lists = [sent_tokenize(i) for i in text]\n",
    "    to_remove = []\n",
    "    for i in range(len(text)): # paras\n",
    "        if text[i] == '':\n",
    "            to_remove.append(text[i])\n",
    "        elif text[i].startswith('– '):\n",
    "            to_remove.append(text[i])\n",
    "        elif len(t_paras_as_lists[i]) < para: # paragraph length filter\n",
    "            to_remove.append(text[i])\n",
    "    for i in to_remove:\n",
    "        text.remove(i)\n",
    "    \n",
    "    # making a shuffled copy\n",
    "    \n",
    "    shuffled_text = []\n",
    "    for i in range(len(text)):\n",
    "        p = []\n",
    "        para_l = sent_tokenize(text[i])\n",
    "        if len(para_l) % 2 == 0:\n",
    "            p.append(None)\n",
    "            for j in range(int(len(para_l) / 2)):\n",
    "                p.append(para_l[j])\n",
    "                p.append(para_l[-(j + 1)])\n",
    "            p[0] = p[-1]\n",
    "            del p[-1]\n",
    "            shuffled_text.append(' '.join(p))\n",
    "        else:\n",
    "            for j in range(int((len(para_l) - 1) / 2)):\n",
    "                p.append(para_l[j])\n",
    "                p.append(para_l[-(j + 1)])\n",
    "            p.append(para_l[j + 1])\n",
    "            shuffled_text.append(' '.join(p))\n",
    "    \n",
    "    # apply index calculations\n",
    "    \n",
    "    ct = 0\n",
    "    for para in text:\n",
    "        if process_para(para, deep, sim_thr, b_sim_thr, ratio_thr, deix, core, same, crit)[0] == True:\n",
    "            ct += 1\n",
    "    \n",
    "    ct_2 = 0\n",
    "    for sh_para in shuffled_text:\n",
    "        if process_para(sh_para, deep, sim_thr, b_sim_thr, ratio_thr, deix, core, same, crit)[0] == False:\n",
    "            ct_2 += 1\n",
    "    \n",
    "    return {'natural': ct / len(text), \n",
    "            'shuffled':  ct_2 / len(shuffled_text), \n",
    "            'total': (ct + ct_2) / (len(text) * 2) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_small(\n",
    "    text, shuffled_text, deep=1, sim_thr=0.4, b_sim_thr=0.78, ratio_thr=0.6, deix=6, core=True, same=True, crit=[0,2,3]):\n",
    "    \n",
    "    # apply index calculations\n",
    "    \n",
    "    ct = 0\n",
    "    for para in text:\n",
    "        if process_para(para, deep, sim_thr, b_sim_thr, ratio_thr, deix, core, same, crit)[0] == True:\n",
    "            ct += 1\n",
    "    \n",
    "    ct_2 = 0\n",
    "    for sh_para in shuffled_text:\n",
    "        if process_para(sh_para, deep, sim_thr, b_sim_thr, ratio_thr, deix, core, same, crit)[0] == False:\n",
    "            ct_2 += 1\n",
    "    \n",
    "    return {'natural': ct / len(text), \n",
    "            'shuffled':  ct_2 / len(shuffled_text), \n",
    "            'total': (ct + ct_2) / (len(text) * 2) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_large(filename, para=4, crit=[0,2,3]):\n",
    "    \n",
    "    # preprocessing\n",
    "    \n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    text = sub('\\n{2,}', '\\n', text)\n",
    "\n",
    "    text = text.split('\\n')\n",
    "    t_paras_as_lists = [sent_tokenize(i) for i in text]\n",
    "    to_remove = []\n",
    "    for i in range(len(text)): # paras\n",
    "        if text[i] == '':\n",
    "            to_remove.append(text[i])\n",
    "        elif text[i].startswith('– '):\n",
    "            to_remove.append(text[i])\n",
    "        elif len(t_paras_as_lists[i]) < para: # paragraph length filter\n",
    "            to_remove.append(text[i])\n",
    "    for i in to_remove:\n",
    "        text.remove(i)\n",
    "\n",
    "    # making a shuffled copy\n",
    "\n",
    "    shuffled_text = []\n",
    "    for i in range(len(text)):\n",
    "        p = []\n",
    "        para_l = sent_tokenize(text[i])\n",
    "        if len(para_l) % 2 == 0:\n",
    "            p.append(None)\n",
    "            for j in range(int(len(para_l) / 2)):\n",
    "                p.append(para_l[j])\n",
    "                p.append(para_l[-(j + 1)])\n",
    "            p[0] = p[-1]\n",
    "            del p[-1]\n",
    "            shuffled_text.append(' '.join(p))\n",
    "        else:\n",
    "            for j in range(int((len(para_l) - 1) / 2)):\n",
    "                p.append(para_l[j])\n",
    "                p.append(para_l[-(j + 1)])\n",
    "            p.append(para_l[j + 1])\n",
    "            shuffled_text.append(' '.join(p))\n",
    "    \n",
    "    # trying different parameter combinations\n",
    "    \n",
    "    results = []\n",
    "    for i in tqdm(range(40, 81, 10)):\n",
    "        sim_thr = i / 100\n",
    "        for j in range(78, 81, 2):\n",
    "            b_sim_thr = j / 100\n",
    "            for k in range(30, 61, 10):\n",
    "                ratio_thr = k / 100\n",
    "#                 if {'sim_thr': sim_thr, 'b_sim_thr': b_sim_thr, 'ratio_thr': ratio_thr} not in [i[0] for i in prev_results]:\n",
    "                prev_results.append(({'sim_thr': sim_thr,\n",
    "                                 'b_sim_thr': b_sim_thr,\n",
    "                                 'ratio_thr': ratio_thr}, \n",
    "                                tuning_small(text, \n",
    "                                             shuffled_text, \n",
    "                                             sim_thr=sim_thr, \n",
    "                                             b_sim_thr=b_sim_thr,\n",
    "                                             ratio_thr=ratio_thr,\n",
    "                                             crit=crit\n",
    "                                            )))\n",
    "#                 else:\n",
    "#                     continue\n",
    "\n",
    "    return sorted(results, key=lambda x: x[1]['total'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'sim_thr': 0.8, 'b_sim_thr': 0.78, 'ratio_thr': 0.5},\n",
       "  {'natural': 0.5333333333333333,\n",
       "   'shuffled': 0.9333333333333333,\n",
       "   'total': 0.7333333333333333}),\n",
       " ({'sim_thr': 0.7, 'b_sim_thr': 0.78, 'ratio_thr': 0.5},\n",
       "  {'natural': 0.5333333333333333,\n",
       "   'shuffled': 0.8666666666666667,\n",
       "   'total': 0.7}),\n",
       " ({'sim_thr': 0.6, 'b_sim_thr': 0.78, 'ratio_thr': 0.4},\n",
       "  {'natural': 0.8,\n",
       "   'shuffled': 0.5333333333333333,\n",
       "   'total': 0.6666666666666666}),\n",
       " ({'sim_thr': 0.8, 'b_sim_thr': 0.78, 'ratio_thr': 0.4},\n",
       "  {'natural': 0.6,\n",
       "   'shuffled': 0.7333333333333333,\n",
       "   'total': 0.6666666666666666}),\n",
       " ({'sim_thr': 0.5, 'b_sim_thr': 0.78, 'ratio_thr': 0.5},\n",
       "  {'natural': 0.8666666666666667,\n",
       "   'shuffled': 0.4666666666666667,\n",
       "   'total': 0.6666666666666666}),\n",
       " ({'sim_thr': 0.6, 'b_sim_thr': 0.78, 'ratio_thr': 0.5},\n",
       "  {'natural': 0.5333333333333333,\n",
       "   'shuffled': 0.8,\n",
       "   'total': 0.6666666666666666}),\n",
       " ({'sim_thr': 0.4, 'b_sim_thr': 0.8, 'ratio_thr': 0.6},\n",
       "  {'natural': 0.6666666666666666,\n",
       "   'shuffled': 0.6,\n",
       "   'total': 0.6333333333333333}),\n",
       " ({'sim_thr': 0.4, 'b_sim_thr': 0.82, 'ratio_thr': 0.6},\n",
       "  {'natural': 0.6666666666666666,\n",
       "   'shuffled': 0.6,\n",
       "   'total': 0.6333333333333333}),\n",
       " ({'sim_thr': 0.6, 'b_sim_thr': 0.8, 'ratio_thr': 0.4},\n",
       "  {'natural': 0.6,\n",
       "   'shuffled': 0.6666666666666666,\n",
       "   'total': 0.6333333333333333}),\n",
       " ({'sim_thr': 0.7, 'b_sim_thr': 0.78, 'ratio_thr': 0.4},\n",
       "  {'natural': 0.6,\n",
       "   'shuffled': 0.6666666666666666,\n",
       "   'total': 0.6333333333333333}),\n",
       " ({'sim_thr': 0.6, 'b_sim_thr': 0.78, 'ratio_thr': 0.3},\n",
       "  {'natural': 0.8,\n",
       "   'shuffled': 0.4666666666666667,\n",
       "   'total': 0.6333333333333333}),\n",
       " ({'sim_thr': 0.8, 'b_sim_thr': 0.8, 'ratio_thr': 0.3},\n",
       "  {'natural': 0.6,\n",
       "   'shuffled': 0.6666666666666666,\n",
       "   'total': 0.6333333333333333}),\n",
       " ({'sim_thr': 0.8, 'b_sim_thr': 0.8, 'ratio_thr': 0.5},\n",
       "  {'natural': 0.3333333333333333,\n",
       "   'shuffled': 0.9333333333333333,\n",
       "   'total': 0.6333333333333333}),\n",
       " ({'sim_thr': 0.4, 'b_sim_thr': 0.78, 'ratio_thr': 0.4},\n",
       "  {'natural': 1.0, 'shuffled': 0.2, 'total': 0.6}),\n",
       " ({'sim_thr': 0.4, 'b_sim_thr': 0.78, 'ratio_thr': 0.6},\n",
       "  {'natural': 0.6666666666666666,\n",
       "   'shuffled': 0.5333333333333333,\n",
       "   'total': 0.6}),\n",
       " ({'sim_thr': 0.6, 'b_sim_thr': 0.8, 'ratio_thr': 0.2},\n",
       "  {'natural': 0.9333333333333333,\n",
       "   'shuffled': 0.26666666666666666,\n",
       "   'total': 0.6}),\n",
       " ({'sim_thr': 0.6, 'b_sim_thr': 0.82, 'ratio_thr': 0.2},\n",
       "  {'natural': 0.9333333333333333,\n",
       "   'shuffled': 0.26666666666666666,\n",
       "   'total': 0.6}),\n",
       " ({'sim_thr': 0.4, 'b_sim_thr': 0.78, 'ratio_thr': 0.5},\n",
       "  {'natural': 0.9333333333333333,\n",
       "   'shuffled': 0.26666666666666666,\n",
       "   'total': 0.6}),\n",
       " ({'sim_thr': 0.5, 'b_sim_thr': 0.78, 'ratio_thr': 0.3},\n",
       "  {'natural': 0.9333333333333333,\n",
       "   'shuffled': 0.26666666666666666,\n",
       "   'total': 0.6}),\n",
       " ({'sim_thr': 0.5, 'b_sim_thr': 0.78, 'ratio_thr': 0.4},\n",
       "  {'natural': 0.9333333333333333,\n",
       "   'shuffled': 0.26666666666666666,\n",
       "   'total': 0.6}),\n",
       " ({'sim_thr': 0.5, 'b_sim_thr': 0.78, 'ratio_thr': 0.6},\n",
       "  {'natural': 0.3333333333333333,\n",
       "   'shuffled': 0.8666666666666667,\n",
       "   'total': 0.6}),\n",
       " ({'sim_thr': 0.5, 'b_sim_thr': 0.8, 'ratio_thr': 0.5},\n",
       "  {'natural': 0.7333333333333333,\n",
       "   'shuffled': 0.4666666666666667,\n",
       "   'total': 0.6}),\n",
       " ({'sim_thr': 0.6, 'b_sim_thr': 0.8, 'ratio_thr': 0.3},\n",
       "  {'natural': 0.7333333333333333,\n",
       "   'shuffled': 0.4666666666666667,\n",
       "   'total': 0.6}),\n",
       " ({'sim_thr': 0.7, 'b_sim_thr': 0.78, 'ratio_thr': 0.3},\n",
       "  {'natural': 0.6666666666666666,\n",
       "   'shuffled': 0.5333333333333333,\n",
       "   'total': 0.6}),\n",
       " ({'sim_thr': 0.7, 'b_sim_thr': 0.8, 'ratio_thr': 0.3},\n",
       "  {'natural': 0.6, 'shuffled': 0.6, 'total': 0.6}),\n",
       " ({'sim_thr': 0.7, 'b_sim_thr': 0.8, 'ratio_thr': 0.5},\n",
       "  {'natural': 0.3333333333333333,\n",
       "   'shuffled': 0.8666666666666667,\n",
       "   'total': 0.6}),\n",
       " ({'sim_thr': 0.8, 'b_sim_thr': 0.78, 'ratio_thr': 0.3},\n",
       "  {'natural': 0.6666666666666666,\n",
       "   'shuffled': 0.5333333333333333,\n",
       "   'total': 0.6}),\n",
       " ({'sim_thr': 0.6, 'b_sim_thr': 0.78, 'ratio_thr': 0.2},\n",
       "  {'natural': 0.9333333333333333,\n",
       "   'shuffled': 0.2,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.6, 'b_sim_thr': 0.78, 'ratio_thr': 0.6},\n",
       "  {'natural': 0.13333333333333333,\n",
       "   'shuffled': 1.0,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.6, 'b_sim_thr': 0.8, 'ratio_thr': 0.6},\n",
       "  {'natural': 0.13333333333333333,\n",
       "   'shuffled': 1.0,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.6, 'b_sim_thr': 0.82, 'ratio_thr': 0.4},\n",
       "  {'natural': 0.4,\n",
       "   'shuffled': 0.7333333333333333,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.6, 'b_sim_thr': 0.82, 'ratio_thr': 0.6},\n",
       "  {'natural': 0.13333333333333333,\n",
       "   'shuffled': 1.0,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.8, 'b_sim_thr': 0.78, 'ratio_thr': 0.2},\n",
       "  {'natural': 0.8,\n",
       "   'shuffled': 0.3333333333333333,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.8, 'b_sim_thr': 0.78, 'ratio_thr': 0.6},\n",
       "  {'natural': 0.13333333333333333,\n",
       "   'shuffled': 1.0,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.8, 'b_sim_thr': 0.8, 'ratio_thr': 0.2},\n",
       "  {'natural': 0.7333333333333333,\n",
       "   'shuffled': 0.4,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.8, 'b_sim_thr': 0.8, 'ratio_thr': 0.6},\n",
       "  {'natural': 0.13333333333333333,\n",
       "   'shuffled': 1.0,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.8, 'b_sim_thr': 0.82, 'ratio_thr': 0.2},\n",
       "  {'natural': 0.7333333333333333,\n",
       "   'shuffled': 0.4,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.8, 'b_sim_thr': 0.82, 'ratio_thr': 0.6},\n",
       "  {'natural': 0.13333333333333333,\n",
       "   'shuffled': 1.0,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.4, 'b_sim_thr': 0.8, 'ratio_thr': 0.5},\n",
       "  {'natural': 0.8666666666666667,\n",
       "   'shuffled': 0.26666666666666666,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.5, 'b_sim_thr': 0.8, 'ratio_thr': 0.3},\n",
       "  {'natural': 0.8666666666666667,\n",
       "   'shuffled': 0.26666666666666666,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.5, 'b_sim_thr': 0.8, 'ratio_thr': 0.4},\n",
       "  {'natural': 0.8,\n",
       "   'shuffled': 0.3333333333333333,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.5, 'b_sim_thr': 0.8, 'ratio_thr': 0.6},\n",
       "  {'natural': 0.26666666666666666,\n",
       "   'shuffled': 0.8666666666666667,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.6, 'b_sim_thr': 0.8, 'ratio_thr': 0.5},\n",
       "  {'natural': 0.3333333333333333,\n",
       "   'shuffled': 0.8,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.7, 'b_sim_thr': 0.78, 'ratio_thr': 0.6},\n",
       "  {'natural': 0.13333333333333333,\n",
       "   'shuffled': 1.0,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.7, 'b_sim_thr': 0.8, 'ratio_thr': 0.6},\n",
       "  {'natural': 0.13333333333333333,\n",
       "   'shuffled': 1.0,\n",
       "   'total': 0.5666666666666667}),\n",
       " ({'sim_thr': 0.4, 'b_sim_thr': 0.78, 'ratio_thr': 0.2},\n",
       "  {'natural': 1.0,\n",
       "   'shuffled': 0.06666666666666667,\n",
       "   'total': 0.5333333333333333}),\n",
       " ({'sim_thr': 0.4, 'b_sim_thr': 0.8, 'ratio_thr': 0.2},\n",
       "  {'natural': 1.0,\n",
       "   'shuffled': 0.06666666666666667,\n",
       "   'total': 0.5333333333333333}),\n",
       " ({'sim_thr': 0.4, 'b_sim_thr': 0.8, 'ratio_thr': 0.4},\n",
       "  {'natural': 0.8666666666666667,\n",
       "   'shuffled': 0.2,\n",
       "   'total': 0.5333333333333333}),\n",
       " ({'sim_thr': 0.4, 'b_sim_thr': 0.82, 'ratio_thr': 0.2},\n",
       "  {'natural': 1.0,\n",
       "   'shuffled': 0.06666666666666667,\n",
       "   'total': 0.5333333333333333}),\n",
       " ({'sim_thr': 0.8, 'b_sim_thr': 0.8, 'ratio_thr': 0.4},\n",
       "  {'natural': 0.3333333333333333,\n",
       "   'shuffled': 0.7333333333333333,\n",
       "   'total': 0.5333333333333333}),\n",
       " ({'sim_thr': 0.8, 'b_sim_thr': 0.82, 'ratio_thr': 0.4},\n",
       "  {'natural': 0.26666666666666666,\n",
       "   'shuffled': 0.8,\n",
       "   'total': 0.5333333333333333}),\n",
       " ({'sim_thr': 0.4, 'b_sim_thr': 0.78, 'ratio_thr': 0.3},\n",
       "  {'natural': 1.0,\n",
       "   'shuffled': 0.06666666666666667,\n",
       "   'total': 0.5333333333333333}),\n",
       " ({'sim_thr': 0.4, 'b_sim_thr': 0.8, 'ratio_thr': 0.3},\n",
       "  {'natural': 1.0,\n",
       "   'shuffled': 0.06666666666666667,\n",
       "   'total': 0.5333333333333333}),\n",
       " ({'sim_thr': 0.7, 'b_sim_thr': 0.8, 'ratio_thr': 0.4},\n",
       "  {'natural': 0.3333333333333333,\n",
       "   'shuffled': 0.7333333333333333,\n",
       "   'total': 0.5333333333333333}),\n",
       " ({'sim_thr': 0.4, 'b_sim_thr': 0.82, 'ratio_thr': 0.4},\n",
       "  {'natural': 0.8,\n",
       "   'shuffled': 0.13333333333333333,\n",
       "   'total': 0.4666666666666667})]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = tuning_large('texts/America_chapter.txt')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordNet proves capable but useless if Word2Vec data is already considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natural': 0.13333333333333333, 'shuffled': 1.0, 'total': 0.5666666666666667}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_text_metric('texts/America_chapter.txt', sim_thr=0.4, ratio_thr=0.6, crit=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natural': 0.6666666666666666, 'shuffled': 0.6, 'total': 0.6333333333333333}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_text_metric('texts/America_chapter.txt', sim_thr=0.4, ratio_thr=0.6, crit=[0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'natural': 0.5333333333333333, 'shuffled': 0.6, 'total': 0.5666666666666667}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_text_metric('texts/America_chapter.txt', sim_thr=0.4, ratio_thr=0.6, crit=[0, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
